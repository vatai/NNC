"""
.. Referenced in: ``custom_layer_test.py``.

An experiment to compare the acurracy of Keras models with and
without compression.

Args:
    model_names (list of strings): A list of model names corresponding to the
        keys of :py:data:`nnclib.model_dict.model_dict`.

    compile_args (dictionary): The dictionary of parameters for model
        compilation such as ``optimizer`` (default value: ``'RMSprop'``),
        ``loss``: (default value: ``'categorical_crossentropy'``), ``metrics``:
        (default value: ``[categorical_accuracy,
        top_k_categorical_accuracy]``).
        See Keras documentation for the `compile function
        <https://keras.io/models/model/#compile>`_.

    gen_args (dictionary): The dictionary of parameters for data generation
        ``img_dir``: location of the images (default value:
        ``expanduser("~/tmp/ilsvrc/db")``),
        ``val_file``: validation file specifying the correct class of each
        image (default value:
        ``expanduser("~/tmp/ilsvrc/caffe_ilsvrc12/val.txt")``),
        ``batch_size``: size of a batch (default value: ``32``),
        ``fast_mode``: process all batches if evaluates to ``False`` (default
        value: ``True``)

    eval_args (dictionary): The dictionary of parameters for the evaluation
        generator.
        ``max_queue_size``: (default value: ``10``),
        ``workers``: (default value ``4``),
        ``use_multiprocessing``: (default value: ``True``),
        ``verbose``: (default value: ``True``).
        For details see Keras documentation `evaluate_generator
        <https://keras.io/models/model/#evaluate_generator>`_.

    proc_args (dictionary): The dictionary of parameters for the
        experiment. For the no processing (to obtain original
        results), set ``proc_args={}``. :
        ``norm``: (default value: ``0``),
        ``epsilon``: (default value: ``0``),
        ``dense_smooth``: (default value: ``0``),
        ``conv_smooth``: (default value: ``0``),
        ``quantization``: (default value: ``0``).

        ``norm``, ``dense_smooth``, ``conv_smooth``, ``quantization`` are
        basically ``bool`` values, but for convenience we use ``0`` and
        ``1`` instead.  These control normalisation, smoothing (melding) of
        dense and conovlutional layers, and quantization respectively.

    seed (int): Random seed (default value ``42``).

Returns:
    files: files are written to the disk in the directory generated by
    :py:func:`nnclib.utils.get_results_dir`.

Notes
-----

The experiment is implemented using the Python package `Sacred
<https://github.com/IDSIA/sacred>`_.  The parameters of the experiment are
provided below and implemented in the :py:meth:`.config` function in the
current file.

The entry point of the experiment is :py:func:`proc_all_models`.

.. todo::

   Finish experiment description.
"""
# flake8: noqa: F841

from os.path import expanduser, join, basename, exists
import json
import numpy as np

from tensorflow import set_random_seed

from keras.layers.convolutional import Conv2D
from keras.layers.core import Dense
from keras.metrics import categorical_accuracy, top_k_categorical_accuracy
from keras.utils import multi_gpu_model

# from sacred.utils import apply_backspaces_and_linefeeds # for progressbars
from sacred import Experiment
from sacred.observers import FileStorageObserver
from sacred.observers import TelegramObserver

from nnclib.compression import reshape_weights
from nnclib.generators import CropGenerator
from nnclib.model_dict import model_dict
from nnclib.utils import get_results_dir


EX = Experiment()
# EX.captured_out_filter = apply_backspaces_and_linefeeds
EX.observers.append(FileStorageObserver.create(get_results_dir(__file__)))
if exists('telegram.json'):
    EX.observers.append(TelegramObserver.from_config('telegram.json'))


@EX.config
def config():
    """Config function for the experiment containing the experiment's
    papameters.

    """
    # pylint: disable=unused-variable
    model_names = ['vgg16']  # list(model_dict.keys())
    compile_args = {'optimizer': 'RMSprop',
                    'loss': 'categorical_crossentropy',
                    'metrics': [categorical_accuracy,
                                top_k_categorical_accuracy]}
    gen_args = {'img_dir': expanduser("~/tmp/ilsvrc/db"),
                'val_file': expanduser("~/tmp/ilsvrc/caffe_ilsvrc12/val.txt"),
                'batch_size': 32,
                'fast_mode': True}
    eval_args = {'max_queue_size': 10,
                 'workers': 4,
                 'use_multiprocessing': True,
                 'verbose': True}
    # For the no processing (original/gold results), set proc_args={}
    proc_args = {'norm': 0,
                 'epsilon': 0,
                 'dense_smooth': 0,
                 'conv_smooth': 0,
                 'quantization': 0}
    seed = 42


def proc_weights(weights, norm, epsilon, quantization, dense_smooth, conv_smooth):
    """
    Process a single layer.

    Applies the following operations:
    """
    # II. norm
    if norm:
        norms = np.linalg.norm(weights, axis=0)
        weights /= norms

    # Pruning (after normalisation).
    weights[np.abs(weights) < epsilon] = 0

    # III. smoothing
    if dense_smooth or conv_smooth:
        sorting = np.argsort(weights, axis=0)
        weights = np.take_along_axis(weights, sorting, axis=0)
        weights = np.mean(weights, axis=1)
        unsort = np.argsort(sorting, axis=0)
        weights = np.take_along_axis(weights[:, np.newaxis], unsort, axis=0)
    # undo: II. norm
    if norm:
        weights *= norms

    if quantization:
        weights = weights.astype(np.float16)

    return weights


@EX.capture
def proc_model(model_name, proc_args=None):
    """
    Process one model based on the model name.  If proc_args is {} or
    None then evaluate all models, as provided by keras, otherwise
    process the Dense layers using some method.
    """
    # because of sacred:
    # pylint: disable=no-value-for-parameter
    model_cls, preproc_args = model_dict[model_name]
    model = model_cls()
    model = multi_gpu_model(model, 2)

    nzcounts = []
    for layer in model.layers[1:]:
        dense_smooth = proc_args['dense_smooth'] and isinstance(layer, Dense)
        conv_smooth = proc_args['conv_smooth'] and isinstance(layer, Conv2D)
        just_epsilon = proc_args['epsilon'] > 0 and \
            not proc_args['dense_smooth'] and \
            not proc_args['conv_smooth'] and \
            isinstance(layer, (Dense, Conv2D))
        if dense_smooth or conv_smooth or just_epsilon:
            weights = layer.get_weights()

            # get_weights() usually returns [weights, bias] if possible we
            # don't want the bias
            # I. unpacking
            weights, rest = weights[0], weights[1:]

            shape = np.shape(weights)  # old shape
            weights = reshape_weights(weights)

            weights = proc_weights(weights, **proc_args)

            # save non-zero count
            nzs = np.count_nonzero(weights, axis=1)
            nzcounts.append([nzs.shape[0], int(nzs[0])])
            # undo: reshape
            weights = np.reshape(weights, shape)
            weights = [weights] + rest

            layer.set_weights(weights)

    result = evaluate(model, preproc_args)
    return result, nzcounts


@EX.capture
def evaluate(model, preproc_args, compile_args, gen_args, eval_args):
    """Evaluate model.

    Args:

        model (model): the model (instantiated) which is to be
            evaluated.  preproc_args (dictionary): provided by the second
            element of values in :py:data:`nnclib.utils.model_dict`.
            compile_args (dictionary): see :py:mod:`combined`.  gen_args
            (dictionary): see :py:mod:`combined`.  eval_args (dictionary):
            see :py:mod:`combined`.

    """
    model.compile(**compile_args)
    gen_args.update(preproc_args)
    generator = CropGenerator(**gen_args)
    result = model.evaluate_generator(generator, **eval_args)
    return result


@EX.automain
def proc_all_models(_seed, model_names, proc_args):
    """The entry point of the experiment. Process all models.  Handle the
    results (weights = number of non-zero weights, accuracy) and write
    them in one of the results files.

    Args:
        _seed (int): random seed.
        model_names (list): list of model names to be processed.
        proc_args (dictionary): see :py:mod:`combined`.

    .. todo::
        Finish ``proc_all_models``.

    """

    set_random_seed(_seed)
    basedir = EX.observers[0].basedir
    print("Basedir {}\n".format(basedir))

    result_template = "_".join(
        [
            "{prefix}",
            "norm{norm}",
            "quant{quantization}",
            "dsmooth{dense_smooth}",
            "csmooth{conv_smooth}",
            "eps{epsilon}",
            "at{basedir}.json"
        ])
    accuracy_file = result_template.format(prefix="accuracy",
                                           basedir=basename(basedir),
                                           **proc_args)
    accuracy_file = join(basedir, accuracy_file)
    weights_file = result_template.format(prefix="weights",
                                          basedir=basename(basedir),
                                          **proc_args)
    weights_file = join(basedir, weights_file)

    accuracy = {}  # aggregate all results in a dictionary
    weights = {}
    for index, name in enumerate(model_names):
        result = proc_model(name)
        # If proc model returned none, then it did nothing so skip.
        if result:
            print(">>>>>> {} - {}/{} Done.".format(name, index + 1,
                                                   len(model_names)))
            print(">>>>>> {} result = {}".format(name, result[0]))
            accuracy[name] = result[0]
            weights[name] = result[1]
        json.dump(accuracy, open(accuracy_file, "w"))
        json.dump(weights, open(weights_file, "w"))
    return proc_args, accuracy, basedir
