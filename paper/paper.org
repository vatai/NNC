#+OPTIONS: ':t toc:nil
#+LATEX_HEADER: \newcommand{\abs}[1]{\lvert #1 \rvert}
#+TITLE: Compression of dense layers in neural networks
#+AUTHOR: Reiji Suda, Emil VATAI

#+begin_abstract
  With the intention to apply HPC methods to ML, we attempted
  compression of layers in deep neural networks, with the goal to
  improve the speed of ML algorithms.  However, in this attempt we
  discovered an interesting property of dense layers.
#+end_abstract

* COMMENT Main points
  - hpc + ml = compression
  - observation of S shape
  - compression approaches
    - sorting -> averaging -> unsorting
    - sparsification
    - quantization
  - compression efficiency
    - float (4byte) or half (2byte) precision

* COMMENT Questions
  - me as an author? UTokyo, ELTE both?

* COMMENT Not to forget
  - Measurements
  - JSPS
  - keras with tf backend, sacred (telegram)
  - github
  - dataset
    
* COMMENT Good paper - notes
** Abstract
   :PROPERTIES:
   :DESCRIPTION: Write last
   :END:
  
   1. The problem
   2. Why is it interesting
   3. What does our solution achieve
   4. What follows from the sollution
** Introduction (1 page)
   1. Describe the problem
   2. State our contribution
  
   Statements (evidence in the body)
** The problem (1 page)
** Our idea (2 page)
** The details (5 page)
** Related work (1-2 pages)
** Conclusion and future work (0.5 page)
* COMMENT --- The paper ---
* Basic idea
  Let $\mathbf{W} \in \mathbb{R}^{m \times n}$ be the weight matrix of a dense layer in a neural network.  
  Let $w_{i,:}$ the \(i\)-th row, and $w_{:,j}$ the \(j\)-th column and $w_{i,j}$ the value in the \(i\)-th row and \(j\)-th column of $\mathbf{W}$.  
  In this case $m$ is the size of the input, and $n$ is the size of the output.
* Assumption 
  Our assumption is that, $\exists \mathcal{S}:\mathbb{N} \to \mathbb{R}$ and
  $\exists{\epsilon} > 0$ so that $\forall i \in \{1, \ldots, m\}$
  - $\exists \alpha_i \in \mathbb{R}^{+}$
  - $\exists \beta_i \in \mathbb{R}$
  - $\exists \pi_i: \{1, \ldots, n\} \to \{1, \ldots, n\}$ permutation
  such that $\forall j: \abs{\mathcal{S}_j - \alpha_i w_{i,\pi_i(j)} + \beta_i} < \epsilon$.
  $\mathcal{S}$ is the "S" shape, to which every row is similar to.

  The three components in the enumeration above can be summarised as a single transformation of a row in a matrix: $f_i(v) = \alpha_i \cdot (v \circ \pi_i) + \beta_i$ (if we consider the vector $v$ as a function with a $\mathbb{N} \to \mathbb{N}$ signature), or simply $f_i(v_j) = \alpha_i v_{\pi_i(j)} + \beta_i$.
   
  From this, it is clear that $f_i$ is characterised by a the triplet
  $(\pi_i, \alpha_i, \beta_i)$, where the permutation $\pi_i$ takes
  the most space (but probably less then the original row $w_{i,:}$).
  Since $f_i$ is a composition of a permutation and a linear
  transformation, it is also invertable, that is $\exists f_i^{-1}$.

  Our idea is to check the performance of the DNN by using
  $f_i^{-1}(\mathcal{S}) = \alpha_i^{-1}(\mathcal{S} \circ
  \pi_i^{-1} - \beta_i)$ instead of $w_{i,:}$, that is substitute
  $w_{i,j}$ with $f_i^{-1}(\mathcal{S}_j) =
  \frac{\mathcal{S}_{\pi_i^{-1}(j)} - \beta_i}{\alpha_i}$ in the hope
  of efficient compression of weights.
