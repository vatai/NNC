#+OPTIONS: ':t toc:nil
#+LATEX_HEADER: \newcommand{\abs}[1]{\lvert #1 \rvert}
#+TITLE: Compression with insight into the layers of deep neural networks
#+AUTHOR: Reiji SUDA, Emil VATAI

#+begin_abstract
  In this paper we explore possible compression schemes of deep neural networks based on the distribution of the weights (parameters) within a layers.
  In the search for an intelligent way to compress deep neural networks, we found an interesting pattern common to dense and convolutional layers.
  All rows of dense layers, after sorting, conform to a single "S-shaped" curve.
  Substituting the original weights, with a single (per column) average (of all the sorted rows), and then "unsorting" them to their original position, results in virtually no reduction of accuracy.
  A after reshaping the weights of convolutional layers into 2D matrices, a similar pattern emerges.
  This characterisation can be used to compress neural networks and hopefully also provides better insight into them. 
  We verify this observation for different pretrained models and measure the compression ratio and accuracy of the networks after compression.
#+end_abstract
   
* COMMENT To SLACK
  Hola! I hope this is not too strange of a question... how much/often do you have dense layers in your models? To be specific, I am not asking about what percentage of layers are dense layers in your models. I.e. a "usual" image recognition model (vgg, resnet) has (let's say) 100 fancy (conv, rnn) layers and one dense layer at the top, only 1%. But and this is true for 99% percent of image recognition models. I am asking about the 99%, so maybe a better phrasing would be "how many of your projects uses at least one dense layer?"
* COMMENT Main points
  - hpc + ml = compression

* COMMENT Questions
  - me as an author? UTokyo, ELTE both?

* COMMENT Not to forget
  - Measurements
  - JSPS
  - keras with tf backend, sacred (telegram)
  - github
  - dataset
    
* COMMENT Good paper - notes
** Abstract
   :PROPERTIES:
   :DESCRIPTION: Write last
   :END:
  
   1. The problem
   2. Why is it interesting
   3. What does our solution achieve
   4. What follows from the sollution
** Introduction (1 page)
   1. Describe the problem
   2. State our contribution
  
   Statements (evidence in the body)
** The problem (1 page)
** Our idea (2 page)
** The details (5 page)
** Related work (1-2 pages)
** Conclusion and future work (0.5 page)
* COMMENT --- The paper ---

* Introduction
  - Neural networks are a sequence of layers of the form $f(Ax+b)$ where $A$ (a matrix) and $b$ are weights, $x$ is the input.
  - We are looking for "meaningful" and/or efficient ways to represent these layer.
  - Compression may lead to better performance.
  - Compression may benefit from a better understanding of layers.
  - We achieved some level of compression.
  - We have achieved a considerable amount of insight.
  - We claim this, based on the fact that our compression improves accuracy of the neural network.
  - We demonstrate this on the pretrained neural networks available for Keras.
* The proposed compression
  - Mathematical description of the S shape.
  - Plots of the S shapes.
  - Stage 1 of compression: Averaging the S shapes.
  - Stage 2 of compression: sparsifying (setting <epsilon values to zero).
  - Stage 3 (in progress): quantization.
  - Normalization
  - Restoration after compression.
  Let $\mathbf{W} \in \mathbb{R}^{m \times n}$ be the weight matrix of a dense layer in a neural network.  
  Let $w_{i,:}$ the \(i\)-th row, and $w_{:,j}$ the \(j\)-th column and $w_{i,j}$ the value in the \(i\)-th row and \(j\)-th column of $\mathbf{W}$.  
  In this case $m$ is the size of the input, and $n$ is the size of the output.
  
  Our assumption is that, $\exists \mathcal{S}:\mathbb{N} \to \mathbb{R}$ and $\exists{\epsilon} > 0$ so that $\forall i \in \{1, \ldots, m\}$
  - $\exists \alpha_i \in \mathbb{R}^{+}$
  - $\exists \beta_i \in \mathbb{R}$
  - $\exists \pi_i: \{1, \ldots, n\} \to \{1, \ldots, n\}$ permutation
  such that $\forall j: \abs{\mathcal{S}_j - \alpha_i w_{i,\pi_i(j)} + \beta_i} < \epsilon$.
  $\mathcal{S}$ is the "S" shape, to which every row is similar to.

  The three components in the enumeration above can be summarised as a single transformation of a row in a matrix: $f_i(v) = \alpha_i \cdot (v \circ \pi_i) + \beta_i$ (if we consider the vector $v$ as a function with a $\mathbb{N} \to \mathbb{N}$ signature), or simply $f_i(v_j) = \alpha_i v_{\pi_i(j)} + \beta_i$.
   
  From this, it is clear that $f_i$ is characterised by a the triplet $(\pi_i, \alpha_i, \beta_i)$, where the permutation $\pi_i$ takes the most space (but probably less then the original row $w_{i,:}$).
  Since $f_i$ is a composition of a permutation and a linear transformation, it is also invertable, that is $\exists f_i^{-1}$.

  Our idea is to check the performance of the DNN by using $f_i^{-1}(\mathcal{S}) = \alpha_i^{-1}(\mathcal{S} \circ \pi_i^{-1} - \beta_i)$ instead of $w_{i,:}$, that is substitute $w_{i,j}$ with $f_i^{-1}(\mathcal{S}_j) = \frac{\mathcal{S}_{\pi_i^{-1}(j)} - \beta_i}{\alpha_i}$ in the hope of efficient compression of weights.
* Results
  - Results of accuracy
  - Plots of accuracy as a function of epsilon
* Conclusion
  - Interpretation - each neuron is good at pushing something, pulling something else, and is irrelevant for most of the things
* Needs to find its place
  - Efficiency of compression
    - Dense layers only
    - Size of the 
    - 32 vs 16 bit floats
  - Connection between "good" models (Resnet) vs "bad" models (vgg)
